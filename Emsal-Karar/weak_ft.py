# -*- coding: utf-8 -*-
"""weak_ft.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D60GZpX3xC2PK3J1Z3HwSQ8maBw-BVO9
"""

!pip -q install sentence-transformers transformers accelerate rank_bm25 tqdm

from google.colab import drive
drive.mount("/content/drive")

# ============================================================
# Emsal Karar Arama — Full Reproducible Comparison (No Drive Models Needed)
#
# Query  : summary_for_model
# BM25   : Facts + Laws
# Dense  : Reasoning (weak triplet training)
# Models : MiniLM, MPNet, BERTurk-Legal, XLM-R (base) + (optional QLoRA)
# Hybrid : alpha * norm(BM25) + (1-alpha) * norm(Dense)
# Metrics: Corpus-level Recall@K
# ============================================================

# ----------------------------
# INSTALL (Colab)
# ----------------------------
# !pip -q install sentence-transformers transformers rank-bm25
# # Optional for QLoRA:
# # !pip -q install peft bitsandbytes accelerate

import os, glob, json, re, random
from typing import Dict, Any, List, Tuple, Optional

import numpy as np
import torch
from torch.utils.data import DataLoader

from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer, InputExample, losses
from sentence_transformers import models as st_models

# Optional QLoRA imports (only used if enabled)
try:
    from peft import LoraConfig, get_peft_model
    from transformers import AutoTokenizer, AutoModel
    PEFT_OK = True
except Exception:
    PEFT_OK = False

# ============================================================
# CONFIG
# ============================================================

DATA_DIR = "/content/drive/MyDrive/Legal NLP/Dataset/Structured_Judgements"   # <-- change
OUT_DIR  = "/content/drive/MyDrive/Legal NLP/Models/compare_runs_nodrive"    # <-- change
os.makedirs(OUT_DIR, exist_ok=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# Training
EPOCHS = 3
BATCH_SIZE = 16
LR = 2e-5
MARGIN = 0.2
MAX_TRAIN = 2000

# Eval
EVAL_SIZE = 300
KS = (1, 5, 10)

# Hybrid
ALPHA = 0.5

# ============================================================
# (1) IO + TEXT UTILS
# ============================================================

def load_json_corpus(data_dir: str, pattern: str = "*.json", recursive: bool = False, limit: Optional[int] = None):
    if recursive and "**" not in pattern:
        pattern = "**/*.json"
    paths = sorted(glob.glob(os.path.join(data_dir, pattern), recursive=recursive))
    if limit is not None:
        paths = paths[:limit]
    raw_objs, doc_ids = [], []
    for p in paths:
        with open(p, "r", encoding="utf-8") as f:
            raw_objs.append(json.load(f))
        doc_ids.append(os.path.basename(p))
    return raw_objs, doc_ids

def clean_text(x: Any) -> str:
    if x is None:
        return ""
    s = str(x)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def tokenize(text: str) -> List[str]:
    return re.findall(r"\w+", text.lower())

def get_summary_for_model(obj: Dict[str, Any]) -> str:
    candidates = []

    for k in ("summary_for_model", "summaryForModel", "model_summary", "summary_model"):
        if k in obj:
            candidates.append(obj.get(k))

    md = obj.get("meta_data") or {}
    if isinstance(md, dict):
        for k in ("summary_for_model", "summaryForModel", "model_summary", "summary_model"):
            if k in md:
                candidates.append(md.get(k))

    summaries = obj.get("summaries") or {}
    if isinstance(summaries, dict):
        for k in ("for_model", "summary_for_model", "model", "model_summary"):
            if k in summaries:
                candidates.append(summaries.get(k))

    rrl = obj.get("rrl_segments") or {}
    if isinstance(rrl, dict):
        for k in ("summary_for_model", "summaryForModel", "model_summary"):
            if k in rrl:
                candidates.append(rrl.get(k))

    for c in candidates:
        s = clean_text(c)
        if s:
            return s
    return ""

# ============================================================
# (2) SEGMENT EXTRACTION
# ============================================================

def extract_segments(obj: Dict[str, Any]) -> Dict[str, str]:
    rrl = obj.get("rrl_segments", {}) or {}
    sf  = obj.get("structural_features", {}) or {}
    md  = obj.get("meta_data", {}) or {}

    facts     = clean_text(rrl.get("facts_text"))
    reasoning = clean_text(rrl.get("reasoning_text"))
    verdict   = clean_text(rrl.get("verdict_text"))
    subject   = clean_text(md.get("case_subject"))

    laws_list = sf.get("mentioned_laws") or []
    if isinstance(laws_list, list):
        laws = " ".join([clean_text(x) for x in laws_list if clean_text(x)])
    else:
        laws = clean_text(laws_list)

    verdict_short = verdict[:600] if verdict else ""
    full = " ".join([facts, reasoning, laws, subject, verdict_short]).strip()

    return {
        "summary_for_model": get_summary_for_model(obj),
        "facts": facts,
        "reasoning": reasoning,
        "laws": laws,
        "subject": subject,
        "verdict_short": verdict_short,
        "full": full,
        "facts_laws": " ".join([facts, laws]).strip(),
        "facts_laws_subject_verdict": " ".join([facts, laws, subject, verdict_short]).strip(),
    }

def build_text(obj: Dict[str, Any], variant: str) -> Tuple[str, str]:
    seg = extract_segments(obj)
    q = seg["summary_for_model"]

    if variant == "summary_to_reasoning":
        return q, seg["reasoning"]
    elif variant == "summary_to_full":
        return q, seg["full"]
    elif variant == "summary_to_facts":
        return q, seg["facts"]
    else:
        raise ValueError(f"Unknown variant: {variant}")

def build_pool(raw_objs, doc_ids, variant: str):
    queries, docs, ids = [], [], []
    missing_q, missing_d = 0, 0
    for obj, did in zip(raw_objs, doc_ids):
        q, d = build_text(obj, variant)
        q = clean_text(q); d = clean_text(d)
        if not q:
            missing_q += 1
        if not d:
            missing_d += 1
        if q and d:
            queries.append(q)
            docs.append(d)
            ids.append(did)
    return queries, docs, ids, missing_q, missing_d

# ============================================================
# (3) SPLIT (DISJOINT)
# ============================================================

def disjoint_split(n: int, eval_size: int, max_train: int, seed: int = 42):
    rng = np.random.default_rng(seed)
    idx = np.arange(n)
    rng.shuffle(idx)

    eval_size_eff = min(eval_size, max(50, n // 10), n)
    eval_idxs = idx[:eval_size_eff].tolist()

    remaining = idx[eval_size_eff:]
    if len(remaining) == 0:
        raise RuntimeError("No train data left after split. Reduce eval_size.")
    train_cap = min(max_train, len(remaining))
    train_idxs = remaining[:train_cap].tolist()

    return train_idxs, eval_idxs

# ============================================================
# (4) BM25 BASELINE (Facts + Laws)
# ============================================================

def build_bm25_index(docs_text: List[str]) -> BM25Okapi:
    tokenized = [tokenize(d) for d in docs_text]
    return BM25Okapi(tokenized)

def bm25_scores(bm25: BM25Okapi, query: str) -> np.ndarray:
    return np.asarray(bm25.get_scores(tokenize(query)), dtype=np.float32)

def recall_at_k_from_scores(scores: np.ndarray, true_idx: List[int], ks=(1,5,10)):
    out = {}
    for k in ks:
        topk = np.argsort(-scores, axis=1)[:, :k]
        hit = [(true_idx[i] in topk[i]) for i in range(len(true_idx))]
        out[f"R@{k}"] = float(np.mean(hit))
    return out

# ============================================================
# (5) DENSE: build encoders
# ============================================================

def make_st_from_hf(hf_id: str, max_seq_length: int = 256, pooling: str = "mean") -> SentenceTransformer:
    word = st_models.Transformer(hf_id, max_seq_length=max_seq_length)
    if pooling == "mean":
        pool = st_models.Pooling(word.get_word_embedding_dimension(), pooling_mode_mean_tokens=True)
    elif pooling == "cls":
        pool = st_models.Pooling(word.get_word_embedding_dimension(), pooling_mode_cls_token=True)
    else:
        raise ValueError("pooling must be 'mean' or 'cls'")
    return SentenceTransformer(modules=[word, pool], device=DEVICE)

def ensure_trainable(st_model: SentenceTransformer):
    trainable = sum(p.requires_grad for p in st_model.parameters())
    total = sum(1 for _ in st_model.parameters())
    print(f"Trainable tensors: {trainable}/{total}")
    if trainable == 0:
        print("⚠️ forcing requires_grad=True on all parameters.")
        for p in st_model.parameters():
            p.requires_grad = True
    st_model.train()

# ============================================================
# (6) HARD NEGATIVES via BM25 on TRAIN DOCS (Reasoning)
# ============================================================

def pick_hard_negative(bm25: BM25Okapi, train_docs: List[str], query: str, pos_idx: int,
                       top_n: int = 50, band_lo: int = 5, band_hi: int = 30) -> int:
    s = bm25.get_scores(tokenize(query))
    ranked = np.argsort(-np.asarray(s))
    ranked = [int(i) for i in ranked[:top_n] if int(i) != int(pos_idx)]
    if not ranked:
        j = pos_idx
        while j == pos_idx:
            j = random.randrange(0, len(train_docs))
        return j
    lo = min(band_lo, len(ranked)-1)
    hi = min(band_hi, len(ranked))
    band = ranked[lo:hi] if hi > lo else ranked
    return random.choice(band)

# ============================================================
# (7) DENSE TRAIN + EVAL (CORPUS-LEVEL ON POOL)
# ============================================================

@torch.no_grad()
def st_encode(model: SentenceTransformer, texts: List[str]) -> np.ndarray:
    return model.encode(texts, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)

@torch.no_grad()
def dense_score_matrix(q_embs: np.ndarray, d_embs: np.ndarray) -> np.ndarray:
    return q_embs @ d_embs.T

def train_weak_triplet(
    model: SentenceTransformer,
    train_queries: List[str],
    train_docs: List[str],
    out_dir: str
):
    bm25 = build_bm25_index(train_docs)

    examples = []
    for i in range(len(train_queries)):
        q = train_queries[i]
        pos = train_docs[i]
        neg_j = pick_hard_negative(bm25, train_docs, q, pos_idx=i)
        neg = train_docs[neg_j]
        examples.append(InputExample(texts=[q, pos, neg]))

    loader = DataLoader(examples, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)
    loss_fn = losses.TripletLoss(model=model, triplet_margin=MARGIN)

    ensure_trainable(model)

    os.makedirs(out_dir, exist_ok=True)
    model.fit(
        train_objectives=[(loader, loss_fn)],
        epochs=EPOCHS,
        optimizer_params={"lr": LR},
        show_progress_bar=True,
        output_path=out_dir
    )
    return model

# ============================================================
# (8) HYBRID (min-max normalize per query)
# ============================================================

def minmax_norm_per_query(scores: np.ndarray, eps: float = 1e-8) -> np.ndarray:
    mn = scores.min(axis=1, keepdims=True)
    mx = scores.max(axis=1, keepdims=True)
    return (scores - mn) / (mx - mn + eps)

def hybrid_scores(bm25_s: np.ndarray, dense_s: np.ndarray, alpha: float) -> np.ndarray:
    b = minmax_norm_per_query(bm25_s)
    d = minmax_norm_per_query(dense_s)
    return alpha * b + (1 - alpha) * d


# ============================================================
# (9) OPTIONAL: QLoRA encoder builder (advanced, only if PEFT_OK)
# This does NOT fully implement QLoRA training end-to-end here.
# It shows how you would wrap an XLM-R backbone with LoRA
# and then place mean pooling on top (ST-style).
# For your paper, if you can't run true QLoRA reliably,
# use "XLM-R (base)" in the comparison.
# ============================================================

def make_xlmr_base_st(max_seq_length=256):
    return make_st_from_hf("xlm-roberta-base", max_seq_length=max_seq_length, pooling="mean")

# ============================================================
# (10) MAIN
# ============================================================

def main():
    raw_objs, doc_ids = load_json_corpus(DATA_DIR, "*.json", recursive=False, limit=None)
    if len(raw_objs) == 0:
        raw_objs, doc_ids = load_json_corpus(DATA_DIR, "**/*.json", recursive=True, limit=None)
    print("Loaded docs:", len(raw_objs))
    assert len(raw_objs) > 0, "No JSON found. Check DATA_DIR."

    # Build split based on summary_to_reasoning pool (for consistent eval)
    variant = "summary_to_reasoning"
    q_all, d_reasoning_all, ids_all, missq, missd = build_pool(raw_objs, doc_ids, variant)
    print(f"[split-pool:{variant}] usable={len(ids_all)} missing_q={missq} missing_d={missd}")

    n = len(ids_all)
    train_idxs, eval_idxs = disjoint_split(n, EVAL_SIZE, MAX_TRAIN, seed=SEED)

    train_queries = [q_all[i] for i in train_idxs]
    train_docs    = [d_reasoning_all[i] for i in train_idxs]

    eval_queries  = [q_all[i] for i in eval_idxs]
    eval_true_idx = eval_idxs  # truth in pool space

    # --- BM25 baseline on Facts+Laws (same eval queries) ---
    bm25_docs = []
    for obj in raw_objs:
        seg = extract_segments(obj)
        bm25_docs.append(seg["facts_laws"])

    bm25 = build_bm25_index(bm25_docs)

    # Map eval queries to their TRUE doc in bm25_docs space:
    # ids_all holds filenames; doc_ids is full corpus filenames order.
    id_to_corpus = {did: i for i, did in enumerate(doc_ids)}
    eval_true_corpus = [id_to_corpus[ids_all[i]] for i in eval_idxs]

    bm25_score_mat = np.stack([bm25_scores(bm25, q) for q in eval_queries], axis=0)
    bm25_metrics = recall_at_k_from_scores(bm25_score_mat, eval_true_corpus, ks=KS)

    print("\n=== BM25 (Facts+Laws) CORPUS metrics ===")
    print(bm25_metrics)

    # --- Dense models (all trained on Reasoning) ---
    model_specs = [
        ("MiniLM", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"),
        ("MPNet",  "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"),
    ]

    # BERTurk-Legal as HF backbone
    hf_specs = [
        ("BERTurk-Legal", "KocLab-Bilkent/BERTurk-Legal"),
    ]

    # XLM-R base (if you don't have QLoRA yet)
    xlmr_specs = [
        ("XLM-R (base)", "xlm-roberta-base"),
    ]

    results = []

    # Precompute pool doc embeddings candidates (reasoning) will be computed per-model
    for name, st_id in model_specs:
        print(f"\n[{name}] Building SentenceTransformer: {st_id}")
        model = SentenceTransformer(st_id, device=DEVICE)

        out_dir = os.path.join(OUT_DIR, f"{name}_weak_summary_to_reasoning")
        model = train_weak_triplet(model, train_queries, train_docs, out_dir)

        # Evaluate on pool (reasoning docs)
        d_embs = st_encode(model, d_reasoning_all)
        q_embs = st_encode(model, eval_queries)
        dense_mat = dense_score_matrix(q_embs, d_embs)
        dense_metrics = recall_at_k_from_scores(dense_mat, eval_true_idx, ks=KS)

        # Hybrid: need BM25 scores on the SAME candidate set in pool space
        # Build BM25 index for pool docs (facts+laws of corresponding ids)
        pool_corpus_idxs = [id_to_corpus[did] for did in ids_all]
        bm25_pool_docs = [bm25_docs[i] for i in pool_corpus_idxs]
        bm25_pool = build_bm25_index(bm25_pool_docs)
        bm25_pool_mat = np.stack([bm25_scores(bm25_pool, q) for q in eval_queries], axis=0)

        hyb_mat = hybrid_scores(bm25_pool_mat, dense_mat, ALPHA)
        hyb_metrics = recall_at_k_from_scores(hyb_mat, eval_true_idx, ks=KS)

        print(f"[{name}] Dense:", dense_metrics)
        print(f"[{name}] Hybrid(alpha={ALPHA}):", hyb_metrics)

        results.append((name, dense_metrics, hyb_metrics, out_dir))

    for name, hf_id in hf_specs:
        print(f"\n[{name}] Building HF->ST backbone: {hf_id}")
        model = make_st_from_hf(hf_id, max_seq_length=256, pooling="mean")

        out_dir = os.path.join(OUT_DIR, f"{name}_weak_summary_to_reasoning")
        model = train_weak_triplet(model, train_queries, train_docs, out_dir)

        d_embs = st_encode(model, d_reasoning_all)
        q_embs = st_encode(model, eval_queries)
        dense_mat = dense_score_matrix(q_embs, d_embs)
        dense_metrics = recall_at_k_from_scores(dense_mat, eval_true_idx, ks=KS)

        pool_corpus_idxs = [id_to_corpus[did] for did in ids_all]
        bm25_pool_docs = [bm25_docs[i] for i in pool_corpus_idxs]
        bm25_pool = build_bm25_index(bm25_pool_docs)
        bm25_pool_mat = np.stack([bm25_scores(bm25_pool, q) for q in eval_queries], axis=0)

        hyb_mat = hybrid_scores(bm25_pool_mat, dense_mat, ALPHA)
        hyb_metrics = recall_at_k_from_scores(hyb_mat, eval_true_idx, ks=KS)

        print(f"[{name}] Dense:", dense_metrics)
        print(f"[{name}] Hybrid(alpha={ALPHA}):", hyb_metrics)

        results.append((name, dense_metrics, hyb_metrics, out_dir))

    for name, hf_backbone in xlmr_specs:
        print(f"\n[{name}] Building HF->ST backbone: {hf_backbone}")
        model = make_st_from_hf(hf_backbone, max_seq_length=256, pooling="mean")

        out_dir = os.path.join(OUT_DIR, f"{name}_weak_summary_to_reasoning")
        model = train_weak_triplet(model, train_queries, train_docs, out_dir)

        d_embs = st_encode(model, d_reasoning_all)
        q_embs = st_encode(model, eval_queries)
        dense_mat = dense_score_matrix(q_embs, d_embs)
        dense_metrics = recall_at_k_from_scores(dense_mat, eval_true_idx, ks=KS)

        pool_corpus_idxs = [id_to_corpus[did] for did in ids_all]
        bm25_pool_docs = [bm25_docs[i] for i in pool_corpus_idxs]
        bm25_pool = build_bm25_index(bm25_pool_docs)
        bm25_pool_mat = np.stack([bm25_scores(bm25_pool, q) for q in eval_queries], axis=0)

        hyb_mat = hybrid_scores(bm25_pool_mat, dense_mat, ALPHA)
        hyb_metrics = recall_at_k_from_scores(hyb_mat, eval_true_idx, ks=KS)

        print(f"[{name}] Dense:", dense_metrics)
        print(f"[{name}] Hybrid(alpha={ALPHA}):", hyb_metrics)

        results.append((name, dense_metrics, hyb_metrics, out_dir))

    print("\n================ FINAL SUMMARY ================")
    print("BM25 (Facts+Laws) :", bm25_metrics)
    for name, dmet, hmet, path in results:
        print(
            f"{name:>14} | Dense R@1={dmet['R@1']:.3f} R@5={dmet['R@5']:.3f} R@10={dmet['R@10']:.3f} "
            f"| Hybrid R@1={hmet['R@1']:.3f} R@5={hmet['R@5']:.3f} R@10={hmet['R@10']:.3f} "
            f"| saved: {path}"
        )

if __name__ == "__main__":
    main()

