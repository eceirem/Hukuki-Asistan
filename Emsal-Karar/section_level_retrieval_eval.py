# -*- coding: utf-8 -*-
"""section_level_retrieval_eval.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CvWB94XYNZ-NWFnJRQW-0eeUdqsnx5uX
"""

from google.colab import drive
drive.mount("/content/drive")

!pip -q install -U sentence-transformers transformers accelerate

# ============================================================
# END-TO-END: Weak Fine-Tuning (Triplet) + CORPUS-LEVEL Recall@K
# Fixes included:
#  1) Disjoint train/eval split (no leakage)
#  2) Corpus-level retrieval eval (queries vs ALL docs)
#  3) Containment leakage fix via facts_to_full_wo_facts variant
#  4) More stable BM25 hard-negative sampling (band sampling)
#  5) Logging missing query/doc counts
# ============================================================

import os, glob, json, re, random
from typing import Dict, Any, List, Tuple, Optional

import numpy as np
import torch
from torch.utils.data import DataLoader

from sentence_transformers import SentenceTransformer, InputExample, losses
from rank_bm25 import BM25Okapi

# ----------------------------
# CONFIG
# ----------------------------
DATA_DIR  = "/content/drive/MyDrive/Legal NLP/Dataset/Structured_Judgements"  # <-- change
SAVE_DIR  = "/content/drive/MyDrive/Legal NLP/Models/weak_ft_runs_fixed"      # <-- change
os.makedirs(SAVE_DIR, exist_ok=True)

BASE_MODEL = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# ============================================================
# (1) IO + TEXT UTILS
# ============================================================

def load_json_corpus(data_dir: str, pattern: str = "*.json", recursive: bool = False, limit: Optional[int] = None):
    if recursive and "**" not in pattern:
        pattern = "**/*.json"
    paths = sorted(glob.glob(os.path.join(data_dir, pattern), recursive=recursive))
    if limit is not None:
        paths = paths[:limit]

    raw_objs, doc_ids = [], []
    for p in paths:
        with open(p, "r", encoding="utf-8") as f:
            raw_objs.append(json.load(f))
        doc_ids.append(os.path.basename(p))
    return raw_objs, doc_ids

def clean_text(x: Any) -> str:
    if x is None:
        return ""
    s = str(x)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def tokenize(text: str) -> List[str]:
    return re.findall(r"\w+", text.lower())

def get_summary_for_model(obj: Dict[str, Any]) -> str:
    candidates = []

    # top-level
    for k in ("summary_for_model", "summaryForModel", "model_summary", "summary_model"):
        if k in obj:
            candidates.append(obj.get(k))

    # meta_data
    md = obj.get("meta_data") or {}
    if isinstance(md, dict):
        for k in ("summary_for_model", "summaryForModel", "model_summary", "summary_model"):
            if k in md:
                candidates.append(md.get(k))

    # summaries
    summaries = obj.get("summaries") or {}
    if isinstance(summaries, dict):
        for k in ("for_model", "summary_for_model", "model", "model_summary"):
            if k in summaries:
                candidates.append(summaries.get(k))

    # rrl_segments (rare)
    rrl = obj.get("rrl_segments") or {}
    if isinstance(rrl, dict):
        for k in ("summary_for_model", "summaryForModel", "model_summary"):
            if k in rrl:
                candidates.append(rrl.get(k))

    for c in candidates:
        s = clean_text(c)
        if s:
            return s
    return ""

# ============================================================
# (2) build_text: VARIANTS (includes leakage-safe full_wo_facts)
# ============================================================

def build_text(obj: Dict[str, Any], variant: str) -> Tuple[str, str]:
    rrl = obj.get("rrl_segments", {}) or {}
    sf  = obj.get("structural_features", {}) or {}
    md  = obj.get("meta_data", {}) or {}

    facts     = clean_text(rrl.get("facts_text"))
    reasoning = clean_text(rrl.get("reasoning_text"))
    verdict   = clean_text(rrl.get("verdict_text"))
    subject   = clean_text(md.get("case_subject"))

    laws_list = sf.get("mentioned_laws") or []
    if isinstance(laws_list, list):
        laws = " ".join([clean_text(x) for x in laws_list if clean_text(x)])
    else:
        laws = clean_text(laws_list)

    summary = get_summary_for_model(obj)
    verdict_short = verdict[:600] if verdict else ""

    full = " ".join([facts, reasoning, laws, subject, verdict_short]).strip()

    # Containment leakage breaker:
    full_wo_facts = " ".join([reasoning, laws, subject, verdict_short]).strip()

    facts_laws_subject_verdict = " ".join([facts, laws, subject, verdict_short]).strip()

    if variant == "summary_to_full":
        q, d = summary, full

    elif variant == "summary_to_facts_laws_subject_verdict":
        q, d = summary, facts_laws_subject_verdict

    elif variant == "summary_to_reasoning":
        q, d = summary, reasoning

    elif variant == "summary_to_facts":
        q, d = summary, facts

    elif variant == "facts_to_full_wo_facts":
        q, d = facts, full_wo_facts

    elif variant == "facts_to_reasoning":
        q, d = facts, reasoning

    else:
        raise ValueError(f"Unknown variant: {variant}")

    return q.strip(), d.strip()

# ============================================================
# (5) RUN WEAK FT (FIXED)
# ============================================================

def run_weak_ft(
    variant: str,
    run_name: str,
    raw_objs,
    doc_ids,
    train_size: int = 2000,
    eval_size: int = 300,
    epochs: int = 1,
    batch_size: int = 16,
    lr: float = 2e-5,
    margin: float = 0.2,
    hardneg_top_n: int = 50,
    hardneg_band_lo: int = 5,
    hardneg_band_hi: int = 30,
    seed: int = 42
):
    rng = np.random.default_rng(seed)

    # Build pool across ALL docs
    queries_all, docs_all, valid_doc_ids = [], [], []
    missing_q, missing_d = 0, 0

    for obj, did in zip(raw_objs, doc_ids):
        q, d = build_text(obj, variant)
        if not q:
            missing_q += 1
        if not d:
            missing_d += 1
        if q and d:
            queries_all.append(q)
            docs_all.append(d)
            valid_doc_ids.append(did)

    n = len(valid_doc_ids)
    print(f"[{variant}] usable={n} | missing_q={missing_q} | missing_d={missing_d}")
    assert n > 50, "Çok az veri kaldı. build_text / JSON alanlarını kontrol et."

    # Disjoint split
    idx_all = np.arange(n)
    rng.shuffle(idx_all)

    eval_size_eff = min(eval_size, max(50, n // 10), n)
    eval_idxs = idx_all[:eval_size_eff].tolist()

    remaining = idx_all[eval_size_eff:]
    if len(remaining) == 0:
        raise RuntimeError("Split sonrası train için veri kalmadı. eval_size'i düşür.")

    train_cap = min(train_size, len(remaining))
    train_idxs = remaining[:train_cap].tolist()

    train_queries = [queries_all[i] for i in train_idxs]
    train_docs    = [docs_all[i]    for i in train_idxs]

    eval_queries  = [queries_all[i] for i in eval_idxs]
    eval_true_idx = eval_idxs  # truth is index in docs_all corpus order

    # Hard negatives via BM25 on TRAIN docs only
    bm25 = build_bm25_index(train_docs)

    examples = []
    for i in range(len(train_queries)):
        q = train_queries[i]
        pos = train_docs[i]
        neg_j = pick_hard_negative(
            bm25, train_docs, q,
            pos_idx=i,
            top_n=hardneg_top_n,
            band_lo=hardneg_band_lo,
            band_hi=hardneg_band_hi
        )
        neg = train_docs[neg_j]
        examples.append(InputExample(texts=[q, pos, neg]))

    # Train
    model = SentenceTransformer(BASE_MODEL, device=DEVICE)
    train_loader = DataLoader(examples, batch_size=batch_size, shuffle=True, drop_last=True)
    train_loss = losses.TripletLoss(model=model, triplet_margin=margin)

    out_dir = os.path.join(SAVE_DIR, run_name)
    os.makedirs(out_dir, exist_ok=True)
    print("Saving to:", out_dir)

    model.fit(
        train_objectives=[(train_loader, train_loss)],
        epochs=epochs,
        optimizer_params={"lr": lr},
        show_progress_bar=True,
        output_path=out_dir
    )

    # CORPUS-LEVEL EVAL: eval_queries vs ALL docs_all
    doc_embs_all = model.encode(
        docs_all,
        convert_to_numpy=True,
        normalize_embeddings=True,
        show_progress_bar=False
    )

    metrics = recall_at_k_corpus(model, eval_queries, doc_embs_all, eval_true_idx, ks=(1, 5, 10))
    print(f"[{variant}] CORPUS metrics:", metrics)

    return out_dir, metrics

# ============================================================
# (6) LOAD DATA
# ============================================================

raw_objs, doc_ids = load_json_corpus(DATA_DIR, pattern="*.json", recursive=False, limit=None)
print("Loaded docs:", len(raw_objs))
if len(raw_objs) == 0:
    raw_objs, doc_ids = load_json_corpus(DATA_DIR, pattern="**/*.json", recursive=True, limit=None)
    print("Loaded docs (recursive):", len(raw_objs))

assert len(raw_objs) > 0, "No JSON files found. Check DATA_DIR."

# ============================================================
# (7) RUN EXPERIMENTS
# ============================================================

variants = [
    "summary_to_full",
    "summary_to_facts_laws_subject_verdict",
    "summary_to_reasoning",
    "summary_to_facts",
    "facts_to_full_wo_facts",   # leakage-safe
    "facts_to_reasoning",
]

results = {}
for v in variants:
    run_name = f"weakft_FIXED_{v}_{BASE_MODEL.split('/')[-1]}"
    out_dir, metrics = run_weak_ft(
        variant=v,
        run_name=run_name,
        raw_objs=raw_objs,
        doc_ids=doc_ids,
        train_size=2000,
        eval_size=300,
        epochs=3,
        batch_size=16,
        lr=2e-5,
        margin=0.2,
        hardneg_top_n=50,
        hardneg_band_lo=5,
        hardneg_band_hi=30,
        seed=SEED
    )
    results[v] = {"out_dir": out_dir, **metrics}

print("\n=== SUMMARY (CORPUS EVAL) ===")
for v in variants:
    r = results[v]
    print(f"{v:>38}  R@1={r['R@1']:.3f}  R@5={r['R@5']:.3f}  R@10={r['R@10']:.3f}  | {r['out_dir']}")