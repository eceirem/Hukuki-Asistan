# -*- coding: utf-8 -*-
"""bm25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12s9_pHRuk_0ZEj4ghqLhAqnYbW6VNCVG
"""

from google.colab import drive
drive.mount("/content/drive")

!pip -q install -U rank-bm25

import os, glob, json, re
from tqdm import tqdm

DATA_DIR = "/content/drive/MyDrive/Legal NLP/Dataset/Structured_Judgements"

def safe_get(d, keys, default=""):
    cur = d
    for k in keys:
        if not isinstance(cur, dict) or k not in cur:
            return default
        cur = cur[k]
    return cur if cur is not None else default

def load_json_objects(data_dir, limit=None):
    paths = sorted(glob.glob(os.path.join(data_dir, "*.json")))
    if limit:
        paths = paths[:limit]

    raw_objs, doc_ids = [], []
    for p in tqdm(paths, desc="Reading JSON"):
        with open(p, "r", encoding="utf-8") as f:
            obj = json.load(f)
        raw_objs.append(obj)
        doc_ids.append(os.path.basename(p))
    return raw_objs, doc_ids

raw_objs, doc_ids = load_json_objects(DATA_DIR, limit=None)
print("Loaded:", len(raw_objs))
print("Sample id:", doc_ids[0])

# ============================================================
# BM25 RETRIEVAL + QUERY GENERATION FROM summary_for_model
# (End-to-end, copy-paste ready)
# ============================================================

import os
import glob
import json
import re
from typing import Dict, Any, List, Tuple, Optional

import numpy as np
from rank_bm25 import BM25Okapi


# ----------------------------
# 1) Load JSON documents
# ----------------------------
def load_json_corpus(data_dir: str, pattern: str = "*.json", limit: Optional[int] = None) -> Tuple[List[Dict[str, Any]], List[str]]:
    """
    Reads JSON files from data_dir and returns:
      raw_objs: list of parsed JSON dicts
      doc_ids : list of file basenames (doc IDs)
    """
    paths = sorted(glob.glob(os.path.join(data_dir, pattern)))
    if limit is not None:
        paths = paths[:limit]

    raw_objs, doc_ids = [], []
    for p in paths:
        with open(p, "r", encoding="utf-8") as f:
            obj = json.load(f)
        raw_objs.append(obj)
        doc_ids.append(os.path.basename(p))
    return raw_objs, doc_ids

# ----------------------------
# 2) Text cleaning + tokenization
# ----------------------------
def clean_text(s: Any) -> str:
    s = "" if s is None else str(s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def tokenize(text: str) -> List[str]:
    # Simple & fast word tokenizer
    return re.findall(r"\w+", text.lower())

# ----------------------------
# 3) Robust extractor for summary_for_model
# ----------------------------
def get_summary_for_model(obj: Dict[str, Any]) -> str:
    """
    Tries multiple likely locations for summary_for_model.
    If your dataset has a different path, add it here (one line).
    """
    candidates = []

    # A) Top-level
    for k in ("summary_for_model", "summaryForModel", "model_summary", "summary_model"):
        if k in obj:
            candidates.append(obj.get(k))

    # B) meta_data
    md = obj.get("meta_data") or {}
    if isinstance(md, dict):
        for k in ("summary_for_model", "summaryForModel", "model_summary", "summary_model"):
            if k in md:
                candidates.append(md.get(k))

    # C) structural_features
    sf = obj.get("structural_features") or {}
    if isinstance(sf, dict):
        for k in ("summary_for_model", "summaryForModel", "model_summary", "summary_model"):
            if k in sf:
                candidates.append(sf.get(k))

    # D) summaries object
    summaries = obj.get("summaries") or {}
    if isinstance(summaries, dict):
        for k in ("for_model", "summary_for_model", "model", "model_summary"):
            if k in summaries:
                candidates.append(summaries.get(k))

    # E) rrl_segments (some datasets store it there)
    rrl = obj.get("rrl_segments") or {}
    if isinstance(rrl, dict):
        for k in ("summary_for_model", "summaryForModel", "model_summary"):
            if k in rrl:
                candidates.append(rrl.get(k))

    # Pick first non-empty cleaned candidate
    for c in candidates:
        txt = clean_text(c)
        if txt:
            return txt

    return ""

# ----------------------------
# 4) Query generation (from summary_for_model)
# ----------------------------
def build_queries_from_summary_for_model(
    raw_objs: List[Dict[str, Any]],
    doc_ids: Optional[List[str]] = None,
    min_chars: int = 60,
    max_chars: int = 350,
) -> Tuple[List[str], List[int], List[str]]:
    """
    Returns:
      queries      : list[str] queries (from summary_for_model)
      valid_idx    : indices into raw_objs (ground-truth doc index per query)
      valid_doc_ids: corresponding doc_ids (if doc_ids provided)
    """
    queries, valid_idx, valid_doc_ids = [], [], []

    for i, obj in enumerate(raw_objs):
        q = clean_text(get_summary_for_model(obj))
        if not q:
            continue

        if len(q) > max_chars:
            q = q[:max_chars].rstrip()

        if len(q) < min_chars:
            continue

        queries.append(q)
        valid_idx.append(i)
        if doc_ids is not None:
            valid_doc_ids.append(doc_ids[i])

    return queries, valid_idx, valid_doc_ids

# ----------------------------
# 5) Build BM25 docs (you can choose modes)
# ----------------------------
def build_bm25_text(obj: Dict[str, Any], mode: str) -> str:
    """
    Builds document text for BM25 indexing using different document representations.
    Adjust keys if your JSON schema differs.
    """
    rrl = obj.get("rrl_segments") or {}
    sf  = obj.get("structural_features") or {}
    md  = obj.get("meta_data") or {}

    facts = clean_text(rrl.get("facts_text"))
    reasoning = clean_text(rrl.get("reasoning_text"))
    verdict = clean_text(rrl.get("verdict_text"))

    laws_list = sf.get("mentioned_laws") or []
    if isinstance(laws_list, list):
        laws = " ".join([clean_text(x) for x in laws_list if clean_text(x)])
    else:
        laws = clean_text(laws_list)

    subject = clean_text(md.get("case_subject"))

    verdict_short = verdict[:600] if verdict else ""

    if mode == "facts":
        return facts
    if mode == "laws":
        return laws
    if mode == "facts_laws":
        return f"{facts} {laws}".strip()
    if mode == "facts_laws_subject":
        return f"{facts} {laws} {subject}".strip()
    if mode == "facts_laws_subject_verdict":
        return f"{facts} {laws} {subject} {verdict_short}".strip()
    if mode == "reasoning":
        return reasoning
    if mode == "full":
        # Full-ish: facts + reasoning + laws + subject + short verdict
        return f"{facts} {reasoning} {laws} {subject} {verdict_short}".strip()

    raise ValueError(f"Unknown mode: {mode}")


def build_bm25(raw_objs: List[Dict[str, Any]], mode: str) -> Tuple[BM25Okapi, List[str]]:
    """
    Returns:
      bm25: fitted BM25Okapi
      docs_text: list[str] doc texts used (for debugging)
    """
    docs_text = []
    tokenized = []

    for obj in raw_objs:
        d = clean_text(build_bm25_text(obj, mode))
        docs_text.append(d)
        tokenized.append(tokenize(d))

    bm25 = BM25Okapi(tokenized)
    return bm25, docs_text

# ----------------------------
# 6) Recall@K evaluation
# ----------------------------
def recall_at_k(bm25: BM25Okapi, queries: List[str], valid_idx: List[int], k: int) -> float:
    hits = 0
    for q, true_i in zip(queries, valid_idx):
        scores = bm25.get_scores(tokenize(q))
        topk = np.argsort(-scores)[:k]
        if true_i in topk:
            hits += 1
    return hits / max(1, len(queries))


def evaluate_modes(
    raw_objs: List[Dict[str, Any]],
    queries: List[str],
    valid_idx: List[int],
    modes: List[str],
    ks: Tuple[int, ...] = (1, 5, 10),
) -> Dict[str, Dict[str, float]]:
    results = {}
    for mode in modes:
        bm25, _ = build_bm25(raw_objs, mode)
        metrics = {}
        for k in ks:
            metrics[f"R@{k}"] = recall_at_k(bm25, queries, valid_idx, k)
        results[mode] = metrics
    return results

# ----------------------------
# 7) Inspect Top-K for one query
# ----------------------------
def show_top_k(
    bm25: BM25Okapi,
    query: str,
    doc_ids: List[str],
    true_index: int,
    k: int = 10
) -> None:
    scores = bm25.get_scores(tokenize(query))
    topk = np.argsort(-scores)[:k]

    print("QUERY:", query[:300], "..." if len(query) > 300 else "")
    print("TRUE DOC:", true_index, doc_ids[true_index])
    print("-" * 80)
    for rank, idx in enumerate(topk, 1):
        print(f"{rank:>2}. score={scores[idx]:.4f} | idx={idx:<4} | doc_id={doc_ids[idx]}")

# ============================================================
# 8) RUN EVERYTHING
# ============================================================
if __name__ == "__main__":
    # ✅ Change this to your dataset folder
    DATA_DIR = "/content/drive/MyDrive/Legal NLP/Dataset/Structured_Judgements"

    # Load corpus
    raw_objs, doc_ids = load_json_corpus(DATA_DIR, pattern="*.json", limit=None)
    print(f"Loaded docs: {len(raw_objs)}")
    if len(raw_objs) == 0:
        raise RuntimeError("No JSON files found. Check DATA_DIR.")

    # Build queries from summary_for_model
    queries, valid_idx, valid_doc_ids = build_queries_from_summary_for_model(
        raw_objs,
        doc_ids=doc_ids,
        min_chars=60,
        max_chars=350,
    )
    print(f"Queries (summary_for_model): {len(queries)} / {len(raw_objs)}")
    if len(queries) == 0:
        print("\n⚠️ No queries produced. Likely summary_for_model path differs in your JSON.")
        print("   Add the correct path inside get_summary_for_model().")
        # Still exit, because evaluation makes no sense without queries.
        raise RuntimeError("No queries produced from summary_for_model.")

    print("Example query:", queries[0][:300])
    print("Example doc_id:", valid_doc_ids[0])

    # Evaluate BM25 across modes (choose what you want)
    modes = [
        "facts",
        "laws",
        "facts_laws",
        "facts_laws_subject",
        "facts_laws_subject_verdict",
        "full",
        "reasoning",
    ]
    results = evaluate_modes(raw_objs, queries, valid_idx, modes=modes, ks=(1, 5, 10))

    print("\nBM25 Results (queries from summary_for_model):")
    for mode in modes:
        m = results[mode]
        print(f"{mode:>26}  R@1={m['R@1']:.3f}  R@5={m['R@5']:.3f}  R@10={m['R@10']:.3f}")

    # Debug: show top-k for the first query using one chosen mode
    chosen_mode = "facts_laws_subject_verdict"
    bm25, _ = build_bm25(raw_objs, chosen_mode)
    print(f"\nTop-K debug (mode={chosen_mode}):")
    show_top_k(bm25, queries[0], doc_ids, true_index=valid_idx[0], k=10)





